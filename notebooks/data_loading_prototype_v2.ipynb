{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing around with data loading approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 is to quickly make a train/validate set from training csv, would like to also keep in mind the ratio of 1:3 class 1 to 0 and maintain that in both train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/home/keil/data/RSNA-pneumonia/stage_1/'\n",
    "train_img = 'stage_1_train_images/'\n",
    "train_csv = 'stage_1_train_labels.csv'\n",
    "######## TEST DUMMY ########\n",
    "dummy_path = '/home/keil/data/RSNA-pneumonia/stage_dummy/'\n",
    "dummy_csv = 'stage_dummy_train_labels.csv'\n",
    "dummy_train = 'dummy_train.csv'\n",
    "dummy_val = 'dummy_val.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive: 6723\n",
      "Negative: 15019\n",
      "Ratio of 1.0 to 3.234 pos to neg\n",
      "\n",
      "Positive: 2241\n",
      "Negative: 5006\n",
      "Ratio of 1.0 to 3.234 pos to neg\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load data csv file into pandas, split into two dataframes for Target = 0/1\n",
    "# Randomly draw 25% from both DFs into two other DFs\n",
    "# Concat back the old DFs and the two new DFs, calling the first train and new one valid\n",
    "# save to csv, giving us both train and validate csvs with a 75/25 split and even class balance between both.\n",
    "# from the EDA notebook:\n",
    "    # Positive: 8964\n",
    "    # Negative: 20025\n",
    "    # Ratio of 1.0 to 3.2 pos to neg\n",
    "\n",
    "# constants\n",
    "shuffle_seed = 10 #to argparse later for the random shuffle and draw\n",
    "pneumonia = 1\n",
    "valid_percent = 0.25\n",
    "\n",
    "# create DFs\n",
    "df = pd.read_csv(dummy_path + dummy_csv)  #df.shape = (28989, 6)\n",
    "df_1 = df[df['Target'] >= pneumonia].reset_index(drop=True) #shape = (8964, 6)\n",
    "df_0 = df[df['Target'] < pneumonia].reset_index(drop=True) #shape = (20025, 6)\n",
    "\n",
    "# Create subsamples of both class DFs with an amount = valid_percent\n",
    "df_1_valid = df_1.sample(frac=valid_percent,random_state=shuffle_seed).sort_index()\n",
    "df_0_valid = df_0.sample(frac=valid_percent,random_state=shuffle_seed).sort_index()\n",
    "\n",
    "# Using the subdsample lets get the symmetric diference or disjoint from the parent sets\n",
    "class_1_diff = df_1.index.symmetric_difference(df_1_valid.index).tolist()\n",
    "class_0_diff = df_0.index.symmetric_difference(df_0_valid.index).tolist()\n",
    "\n",
    "# Create our training DFs based on that subset from above\n",
    "df_1_train = df_1.iloc[class_1_diff]\n",
    "df_0_train = df_0.iloc[class_0_diff]\n",
    "\n",
    "# Check that our subset DFs for train and valid are equal in size to df_1 and df_0\n",
    "assert df_1_valid.shape[0] + df_1_train.shape[0] == df_1.shape[0]\n",
    "assert df_0_valid.shape[0] + df_0_train.shape[0] == df_0.shape[0]\n",
    "\n",
    "#concat DFs\n",
    "df_train = pd.concat([df_1_train, df_0_train])\n",
    "df_valid = pd.concat([df_1_valid, df_0_valid])\n",
    "\n",
    "#check final shapes\n",
    "assert df_train.shape[0] + df_valid.shape[0] == df.shape[0]\n",
    "\n",
    "#Write out DFs to CSVs\n",
    "df_train.to_csv(dummy_path + 'dummy_train.csv',index=False)\n",
    "df_valid.to_csv(dummy_path + 'dummy_val.csv',index=False)\n",
    "\n",
    "for _ in [df_train,df_valid]:\n",
    "    label_bool = _['Target'].tolist()\n",
    "    data_count = len(label_bool)\n",
    "    positives = np.sum(label_bool)\n",
    "    print('Positive: {}\\nNegative: {}'.format(positives,(data_count-positives)))\n",
    "    print('Ratio of {} to {} pos to neg\\n'.format(positives/positives,np.round(data_count/positives,3)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 is to get DICOM images into tf.data.dataset for training..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/keil/data/RSNA-pneumonia/stage_1/stage_1_train_images/00436515-870c-4b36-a041-de91049b9ab4.dcm\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "DATA_PATH = '/home/keil/data/RSNA-pneumonia/stage_1/'\n",
    "IMG_DIR = 'stage_1_train_images/'\n",
    "CSV_PATH = DATA_PATH + 'train.csv'\n",
    "\n",
    "def split_data_labels(csv_path, path):\n",
    "    \"\"\" take CSVs with filepaths/labels and extracts them into parallel lists\"\"\"\n",
    "    filenames = []\n",
    "    labels = []\n",
    "    with open(csv_path, 'r') as f:\n",
    "        next(f)\n",
    "        for line in f:\n",
    "            new_line = line.strip().split(',')\n",
    "            #[0]=patientID (same as DICOM name) [5]=Target\n",
    "            filenames.append(path + new_line[0]+'.dcm')\n",
    "            labels.append(int(new_line[5])) #DEBUG float??? was float before\n",
    "    return filenames,labels\n",
    "\n",
    "train_imgs, train_labels = split_data_labels(CSV_PATH, DATA_PATH+IMG_DIR)\n",
    "\n",
    "assert len(train_imgs) == len(train_labels)\n",
    "print(train_imgs[0])\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1024, 1024)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = pydicom.dcmread(train_imgs[0])\n",
    "image = ds.pixel_array\n",
    "print(type(image))\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/keil/data/RSNA-pneumonia/stage_1/stage_1_train_images/00436515-870c-4b36-a041-de91049b9ab4.dcm\n"
     ]
    }
   ],
   "source": [
    "def build_dataset(data, labels):\n",
    "    \"\"\"todo\"\"\"\n",
    "    labels = tf.one_hot(tf.cast(labels, tf.uint8), 1) #cast labels to dim 2 tf obj\n",
    "#     print(labels)\n",
    "    #data = pydicom.dcmread(data).pixel_array\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((data, labels))\n",
    "#     dataset = dataset.shuffle(len(data))\n",
    "#     dataset = dataset.repeat()\n",
    "    dataset = dataset.map(decode)\n",
    "    dataset = dataset.map(preprocess_img, num_parallel_calls=2)\n",
    "    # dataset = dataset.map(img_augmentation, num_parallel_calls=2)\n",
    "#     dataset = dataset.batch(BATCH_SIZE) # (?, x, y) unknown batch size because the last batch will have fewer elements.\n",
    "#     dataset = dataset.prefetch(PREFETCH_SIZE) #single training step consumes n elements\n",
    "    print(data[0])\n",
    "    return dataset\n",
    "\n",
    "def dicom_to_np(filename):\n",
    "#     image_string = tf.read_file(filename)\n",
    "    ds = pydicom.dcmread(filename)\n",
    "    image = ds.pixel_array\n",
    "#     print('ji'*50)\n",
    "#     print(image.shape)\n",
    "    return image.astype(np.float32)\n",
    "\n",
    "def decode(filename, label):\n",
    "    # input = tf.placeholder(tf.float32)\n",
    "    image_string = tf.read_file(filename)\n",
    "    return tf.py_func(dicom_to_np, [image_string], tf.float32), label\n",
    "\n",
    "def preprocess_img(img, label):\n",
    "    image = tf.image.convert_image_dtype(img, tf.float32) #convert to float values in [0, 1]\n",
    "#     image = tf.image.resize_images(image, [1024, 1024])\n",
    "    return image, label\n",
    "\n",
    "train_dataset = build_dataset(train_imgs, train_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<ParallelMapDataset shapes: (<unknown>, (1,)), types: (tf.float32, tf.float32)>\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnknownError",
     "evalue": "AttributeError: 'bytes' object has no attribute 'read'\nTraceback (most recent call last):\n\n  File \"/home/keil/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 158, in __call__\n    ret = func(*args)\n\n  File \"<ipython-input-25-cdc0b6753722>\", line 19, in dicom_to_np\n    ds = pydicom.dcmread(filename)\n\n  File \"/home/keil/miniconda3/envs/tensorflow/lib/python3.6/site-packages/pydicom/filereader.py\", line 886, in dcmread\n    force=force, specific_tags=specific_tags)\n\n  File \"/home/keil/miniconda3/envs/tensorflow/lib/python3.6/site-packages/pydicom/filereader.py\", line 689, in read_partial\n    preamble = read_preamble(fileobj, force)\n\n  File \"/home/keil/miniconda3/envs/tensorflow/lib/python3.6/site-packages/pydicom/filereader.py\", line 627, in read_preamble\n    preamble = fp.read(128)\n\nAttributeError: 'bytes' object has no attribute 'read'\n\n\n\t [[Node: PyFunc = PyFunc[Tin=[DT_STRING], Tout=[DT_FLOAT], token=\"pyfunc_11\"](ReadFile)]] [Op:IteratorGetNextSync]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnknownError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-53e120468e53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0miterator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_one_shot_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# For Python 3 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36mnext\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \"\"\"\n\u001b[1;32m    517\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/data/ops/iterator_ops.py\u001b[0m in \u001b[0;36m_next_internal\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    506\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m             \u001b[0moutput_types\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m             output_shapes=self._flat_output_shapes)\n\u001b[0m\u001b[1;32m    509\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m       return sparse.deserialize_sparse_tensors(\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36miterator_get_next_sync\u001b[0;34m(iterator, output_types, output_shapes, name)\u001b[0m\n\u001b[1;32m   1857\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1858\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1859\u001b[0;31m       \u001b[0m_six\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1860\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1861\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mUnknownError\u001b[0m: AttributeError: 'bytes' object has no attribute 'read'\nTraceback (most recent call last):\n\n  File \"/home/keil/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/script_ops.py\", line 158, in __call__\n    ret = func(*args)\n\n  File \"<ipython-input-25-cdc0b6753722>\", line 19, in dicom_to_np\n    ds = pydicom.dcmread(filename)\n\n  File \"/home/keil/miniconda3/envs/tensorflow/lib/python3.6/site-packages/pydicom/filereader.py\", line 886, in dcmread\n    force=force, specific_tags=specific_tags)\n\n  File \"/home/keil/miniconda3/envs/tensorflow/lib/python3.6/site-packages/pydicom/filereader.py\", line 689, in read_partial\n    preamble = read_preamble(fileobj, force)\n\n  File \"/home/keil/miniconda3/envs/tensorflow/lib/python3.6/site-packages/pydicom/filereader.py\", line 627, in read_preamble\n    preamble = fp.read(128)\n\nAttributeError: 'bytes' object has no attribute 'read'\n\n\n\t [[Node: PyFunc = PyFunc[Tin=[DT_STRING], Tout=[DT_FLOAT], token=\"pyfunc_11\"](ReadFile)]] [Op:IteratorGetNextSync]"
     ]
    }
   ],
   "source": [
    "\n",
    "iterator = train_dataset.make_one_shot_iterator()\n",
    "for x in iterator:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling Model!\n",
      "Beginning to Train Model\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_2 to have 2 dimensions, but got array with shape ()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-97fb52db8d2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m#3197 validation number\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         callbacks=None)  #https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1261\u001b[0m         \u001b[0msteps_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'steps_per_epoch'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m         validation_split=validation_split)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, batch_size, check_steps, steps_name, steps, validation_split)\u001b[0m\n\u001b[1;32m    905\u001b[0m           \u001b[0mfeed_output_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m           \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 907\u001b[0;31m           exception_prefix='target')\n\u001b[0m\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    180\u001b[0m                            \u001b[0;34m': expected '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' to have '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m                            \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' dimensions, but got array '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m                            'with shape ' + str(data_shape))\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m           \u001b[0mdata_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking target: expected dense_2 to have 2 dimensions, but got array with shape ()"
     ]
    }
   ],
   "source": [
    "DenseNet169 = tf.keras.applications.densenet.DenseNet169(include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=None,\n",
    "        input_shape=(1024, 1024, 3),\n",
    "        pooling='max',\n",
    "        classes=2)\n",
    "last_layer = DenseNet169.output\n",
    "# print(last_layer)\n",
    "preds = tf.keras.layers.Dense(1, activation='sigmoid')(last_layer)\n",
    "model = tf.keras.Model(DenseNet169.input, preds)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=1,\n",
    "        beta1=1,\n",
    "        beta2=1)\n",
    "\n",
    "optimizer_keras = tf.keras.optimizers.Adam(lr=1,\n",
    "        beta_1=1,\n",
    "        beta_2=1,\n",
    "        decay=0.10)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint\n",
    "checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath='./',\n",
    "        monitor=\"val_loss\",\n",
    "        verbose=1,\n",
    "        save_best_only=True)\n",
    "\n",
    "# https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/TensorBoard\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir='./',\n",
    "        # histogram_freq=1, #this screwed us over... caused tensorboard callback to fail.. why??? DEBUG !!!!!!\n",
    "        # batch_size=BATCH_SIZE, # and take this out... and boom.. histogam frequency works. sob\n",
    "        write_graph=True,\n",
    "        write_grads=False,\n",
    "        write_images=True)\n",
    "\n",
    "print(\"Compiling Model!\")\n",
    "model.compile(optimizer=optimizer,\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "print(\"Beginning to Train Model\")\n",
    "model.fit(train_dataset,\n",
    "        epochs=1,\n",
    "        steps_per_epoch=(len(train_labels)//3), #36808 train number\n",
    "        verbose=1,\n",
    "        validation_data=None,\n",
    "        validation_steps=None,  #3197 validation number\n",
    "        callbacks=None)  #https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TextLineDataset(data_path+train_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ShuffleDataset shapes: (), types: tf.string>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.list_files('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.data.ops.iterator_ops.Iterator'>\n"
     ]
    }
   ],
   "source": [
    "tmp = dataset.make_one_shot_iterator()\n",
    "print(type(tmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'method' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-8145be2283eb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# sess.run(iterator.initializer)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'method' object is not iterable"
     ]
    }
   ],
   "source": [
    "# sess.run(iterator.initializer)\n",
    "for x in tmp:\n",
    "    print(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
